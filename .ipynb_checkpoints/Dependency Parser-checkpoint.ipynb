{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import sys\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines parsed:  242774 of the 242774\n",
      "Number of sentences: 12542 \n",
      "\n",
      "Example of datastructure: \n",
      "\n",
      " [[{0: '1', 1: 'Al', 3: 'PROPN', 6: '0', 7: 'root'}, {0: '2', 1: '-', 3: 'PUNCT', 6: '1', 7: 'punct'}, {0: '3', 1: 'Zaman', 3: 'PROPN', 6: '1', 7: 'flat'}, {0: '4', 1: ':', 3: 'PUNCT', 6: '1', 7: 'punct'}, {0: '5', 1: 'American', 3: 'ADJ', 6: '6', 7: 'amod'}, {0: '6', 1: 'forces', 3: 'NOUN', 6: '7', 7: 'nsubj'}, {0: '7', 1: 'killed', 3: 'VERB', 6: '1', 7: 'parataxis'}, {0: '8', 1: 'Shaikh', 3: 'PROPN', 6: '7', 7: 'obj'}, {0: '9', 1: 'Abdullah', 3: 'PROPN', 6: '8', 7: 'flat'}, {0: '10', 1: 'al', 3: 'PROPN', 6: '8', 7: 'flat'}, {0: '11', 1: '-', 3: 'PUNCT', 6: '8', 7: 'punct'}, {0: '12', 1: 'Ani', 3: 'PROPN', 6: '8', 7: 'flat'}, {0: '13', 1: ',', 3: 'PUNCT', 6: '8', 7: 'punct'}, {0: '14', 1: 'the', 3: 'DET', 6: '15', 7: 'det'}, {0: '15', 1: 'preacher', 3: 'NOUN', 6: '8', 7: 'appos'}, {0: '16', 1: 'at', 3: 'ADP', 6: '18', 7: 'case'}, {0: '17', 1: 'the', 3: 'DET', 6: '18', 7: 'det'}, {0: '18', 1: 'mosque', 3: 'NOUN', 6: '7', 7: 'obl'}, {0: '19', 1: 'in', 3: 'ADP', 6: '21', 7: 'case'}, {0: '20', 1: 'the', 3: 'DET', 6: '21', 7: 'det'}, {0: '21', 1: 'town', 3: 'NOUN', 6: '18', 7: 'nmod'}, {0: '22', 1: 'of', 3: 'ADP', 6: '23', 7: 'case'}, {0: '23', 1: 'Qaim', 3: 'PROPN', 6: '21', 7: 'nmod'}, {0: '24', 1: ',', 3: 'PUNCT', 6: '21', 7: 'punct'}, {0: '25', 1: 'near', 3: 'ADP', 6: '28', 7: 'case'}, {0: '26', 1: 'the', 3: 'DET', 6: '28', 7: 'det'}, {0: '27', 1: 'Syrian', 3: 'ADJ', 6: '28', 7: 'amod'}, {0: '28', 1: 'border', 3: 'NOUN', 6: '21', 7: 'nmod'}, {0: '29', 1: '.', 3: 'PUNCT', 6: '1', 7: 'punct'}], [{0: '1', 1: '[', 3: 'PUNCT', 6: '10', 7: 'punct'}, {0: '2', 1: 'This', 3: 'DET', 6: '3', 7: 'det'}, {0: '3', 1: 'killing', 3: 'NOUN', 6: '10', 7: 'nsubj'}, {0: '4', 1: 'of', 3: 'ADP', 6: '7', 7: 'case'}, {0: '5', 1: 'a', 3: 'DET', 6: '7', 7: 'det'}, {0: '6', 1: 'respected', 3: 'ADJ', 6: '7', 7: 'amod'}, {0: '7', 1: 'cleric', 3: 'NOUN', 6: '3', 7: 'nmod'}, {0: '8', 1: 'will', 3: 'AUX', 6: '10', 7: 'aux'}, {0: '9', 1: 'be', 3: 'AUX', 6: '10', 7: 'aux'}, {0: '10', 1: 'causing', 3: 'VERB', 6: '0', 7: 'root'}, {0: '11', 1: 'us', 3: 'PRON', 6: '10', 7: 'iobj'}, {0: '12', 1: 'trouble', 3: 'NOUN', 6: '10', 7: 'obj'}, {0: '13', 1: 'for', 3: 'ADP', 6: '14', 7: 'case'}, {0: '14', 1: 'years', 3: 'NOUN', 6: '10', 7: 'obl'}, {0: '15', 1: 'to', 3: 'PART', 6: '16', 7: 'mark'}, {0: '16', 1: 'come', 3: 'VERB', 6: '14', 7: 'acl'}, {0: '17', 1: '.', 3: 'PUNCT', 6: '10', 7: 'punct'}, {0: '18', 1: ']', 3: 'PUNCT', 6: '10', 7: 'punct'}]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.path.join(\"data\",\"en-ud-train.conllu\")\n",
    "\n",
    "def read(path):\n",
    "    '''Constructs a list(corpus) of lists(sentences) of dicts(relevant information per word)'''\n",
    "    with open(path, 'r', encoding = 'utf8') as f:\n",
    "        \n",
    "        data =[]\n",
    "        \n",
    "        # Needed to select sentences\n",
    "        first_sent = True  \n",
    "        previous_line_hashtag = False\n",
    "        \n",
    "        #linenumber for debugging\n",
    "        linenumber = 0\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            #empty linedict\n",
    "            linedict = {}\n",
    "            linenumber +=1\n",
    "            \n",
    "            # Skip newdoc id and skip sentence id. Skip whitelines\n",
    "            if line[0] in ['#','r\\n','\\n']:\n",
    "                if line[0] in ['#']:\n",
    "                    previous_line_hashtag = True                    \n",
    "                continue\n",
    "                \n",
    "            \n",
    "            # empty list for sentence\n",
    "            if (line[0] == '1' and previous_line_hashtag == True):\n",
    "                previous_line_hashtag = False\n",
    "                if first_sent == True:\n",
    "                    sentence = []\n",
    "                    first_sent = False\n",
    "                else:\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "            \n",
    "            # strip tabs\n",
    "            parts = line.strip().split(\"\\t\") \n",
    "            \n",
    "            # Make sure that all lines have the same structure \n",
    "            if len(parts) != 10:\n",
    "                print(linenumber)\n",
    "                print(parts)\n",
    "                print('not same structure')\n",
    "                break\n",
    "            \n",
    "            # fill linedict\n",
    "            for i, part in enumerate(parts):\n",
    "                \n",
    "                # only save the relevant parts\n",
    "                if i not in [0,1,3,6,7]:\n",
    "                    continue\n",
    "                linedict[i] = part\n",
    "            \n",
    "            sentence.append(linedict)\n",
    "\n",
    "        # Check if it parsed all lines\n",
    "        print('lines parsed: ',linenumber, 'of the 242774')\n",
    "        return data\n",
    "\n",
    "eng_train = read(path)\n",
    "print('Number of sentences:', len(eng_train),'\\n')\n",
    "print('Example of datastructure: \\n\\n', eng_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: '<root>', 1: 'Al', 2: '-', 3: 'Zaman', 4: ':', 5: 'American', 6: 'forces', 7: 'killed', 8: 'Shaikh', 9: 'Abdullah', 10: 'al', 11: '-', 12: 'Ani', 13: ',', 14: 'the', 15: 'preacher', 16: 'at', 17: 'the', 18: 'mosque', 19: 'in', 20: 'the', 21: 'town', 22: 'of', 23: 'Qaim', 24: ',', 25: 'near', 26: 'the', 27: 'Syrian', 28: 'border', 29: '.'}, {0: '<root>', 1: '[', 2: 'This', 3: 'killing', 4: 'of', 5: 'a', 6: 'respected', 7: 'cleric', 8: 'will', 9: 'be', 10: 'causing', 11: 'us', 12: 'trouble', 13: 'for', 14: 'years', 15: 'to', 16: 'come', 17: '.', 18: ']'}, {0: '<root>', 1: 'DPA', 2: ':', 3: 'Iraqi', 4: 'authorities', 5: 'announced', 6: 'that', 7: 'they', 8: 'had', 9: 'busted', 10: 'up', 11: '3', 12: 'terrorist', 13: 'cells', 14: 'operating', 15: 'in', 16: 'Baghdad', 17: '.'}]\n"
     ]
    }
   ],
   "source": [
    "# Construct sentences from the training data, default (for development purposes) is 1 sentence\n",
    "def construct_sentences(corpus, n=1):\n",
    "    '''Constructs a list of dicts(sentences) with words and indices of the words (and the <root> symbol)'''\n",
    "    \n",
    "    sentences = []\n",
    "    n=3\n",
    "\n",
    "    \n",
    "    for sent in corpus[0:n]:\n",
    "        # create mappings from index to words for faster computability\n",
    "        i2words = {}\n",
    "        \n",
    "        for i, word in enumerate(sent):\n",
    "#             Append 'root word'\n",
    "            if word[0]=='1':\n",
    "                i2words[i] = '<root>'\n",
    "                \n",
    "            i2words[i+1] = word[1]\n",
    "            \n",
    "        sentences.append(i2words)\n",
    "    return sentences\n",
    "        \n",
    "        \n",
    "sentences = construct_sentences(eng_train)\n",
    "print(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-118-366fa06800b3>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-118-366fa06800b3>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    graph[to_index] = [(i: random.random()) for i in from_index]\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def construct_graph(sentences, n=1):\n",
    "    '''Constructs a list of dicts(sentences) where each sentence is represented as a graph,\n",
    "    inspired by the Python documentation recommended structure (https://www.python.org/doc/essays/graphs/). \n",
    "    \n",
    "    graphs = [graph_of_sentence_1, graph_of_sentnce_2,...]\n",
    "    \n",
    "    Where each graph has the structure:\n",
    "        \n",
    "    graph_i = {to (index of a word within a sentence): [(from, weight), (from, weight)],\n",
    "             2: [(1, 0.5323), (3, 0.3452345),....,(n, weight)],\n",
    "             ...}\n",
    "    \n",
    "             \n",
    "    '''\n",
    "    \n",
    "    graphs = []\n",
    "    graph = {}\n",
    "    n=2\n",
    "    \n",
    "    # Create (fully conected) graph \n",
    "    # TODO: (with random weights at the moment, should change that to the weights given by the Neural Net)\n",
    "    for sent in sentences[0:n]:\n",
    "        \n",
    "        length = len(sent)  \n",
    "        for to_index, word in sent.items():\n",
    "            \n",
    "            # Make sure there is nog connection from a node to itself\n",
    "            #TODO: Volgens mij hoeven we maar 1 keer de probabilities van elke arc er in te stoppen (dit wordt niet geupdate)\n",
    "            # Daarom gebruik ik hieronder tuples, maar anders moeten we misschien iets anders gebruiken\n",
    "            from_index = [i for i in range(length) if i != to_index]\n",
    "            graph[to_index] = [(i: random.random()) for i in from_index]\n",
    "            \n",
    "        graphs.append(graph)\n",
    "        \n",
    "    return graphs\n",
    "             \n",
    "graphs = construct_graph(sentences)\n",
    "print(graphs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-07dcefba1eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mMST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-07dcefba1eb8>\u001b[0m in \u001b[0;36mMST\u001b[0;34m(graphs, n)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[1;31m# find maximum incomming arcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincomming_arcs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mmaximum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincomming_arcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mincomming_arcs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marcs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-07dcefba1eb8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[1;31m# find maximum incomming arcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincomming_arcs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mmaximum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincomming_arcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mincomming_arcs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marcs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-07dcefba1eb8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[1;31m# find maximum incomming arcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincomming_arcs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mmaximum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincomming_arcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mincomming_arcs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marcs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "def MST(graphs, n=1):\n",
    "    # n=1 for development purposes\n",
    "    '''Takes a list of graphs and returns the Maximum Spanning Tree per graph by using the Chu Liu Edmonds Algorithm'''\n",
    "\n",
    "    for sentence in graphs[0:n]:\n",
    "        # find maximum incomming arcs\n",
    "        for word_index, incomming_arcs in sentence.items():\n",
    "            \n",
    "            break\n",
    "        \n",
    "        # find all cycles\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "MST(graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
